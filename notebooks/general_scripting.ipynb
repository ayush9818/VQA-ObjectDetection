{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import clip \n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/clip_vqa/extract_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/clip_vqa/extract_features.py\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import clip \n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def load_clip(model_name='RN50', device='cpu'):\n",
    "    clip_model, preprocess = clip.load(model_name, device=device)\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\n",
    "    print(f\"Input resolution: {clip_model.visual.input_resolution}\")\n",
    "    print(f\"Context length: {clip_model.context_length}\")\n",
    "    print(f\"Vocab size: {clip_model.vocab_size}\")\n",
    "    return clip_model, preprocess\n",
    "\n",
    "def extract_features(image_path, question, clip_model, transform, device='cpu'):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "    question = clip.tokenize(question).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        text_features = clip_model.encode_text(question)\n",
    "    return image_features.detach().cpu(), text_features.detach().cpu()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data-path\",type=str)\n",
    "    parser.add_argument(\"--feat-save-dir\",type=str)\n",
    "    parser.add_argument(\"--clip-model\", type=str, default='RN50')\n",
    "    parser.add_argument(\"--device\", type=str, default='cuda:0')\n",
    "    parser.add_argument(\"--save-path\",type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    assert os.path.exists(args.data_path)\n",
    "\n",
    "    feat_save_dir = args.feat_save_dir\n",
    "    device=args.device\n",
    "    os.makedirs(feat_save_dir, exist_ok=True)\n",
    "    clip_model_name = args.clip_model\n",
    "    save_path = args.save_path\n",
    "\n",
    "    clip_model, preprocess = load_clip(model_name='RN50', device=device)\n",
    "\n",
    "    data = pd.read_json(args.data_path)\n",
    "\n",
    "    img_feat_list = []\n",
    "    text_feat_list = []\n",
    "\n",
    "    total = len(data)\n",
    "    done = 0\n",
    "    for idx,row in data.iterrows():\n",
    "        image_path = row['image_path']\n",
    "        question = row['question']\n",
    "        text_feat_name = row['image_name'].split('.')[0] + '_text.pt'\n",
    "        img_feat_name = row['image_name'].split('.')[0] + '_img.pt'\n",
    "        \n",
    "        img_feat, text_feat = extract_features(image_path=image_path,\n",
    "                                       question=question,\n",
    "                                       clip_model=clip_model,\n",
    "                                       transform=preprocess,\n",
    "                                       device=device\n",
    "                                    )\n",
    "        img_feat_path = os.path.join(feat_save_dir,img_feat_name)\n",
    "        text_feat_path = os.path.join(feat_save_dir,text_feat_name)\n",
    "        torch.save(img_feat, img_feat_path)\n",
    "        torch.save(text_feat, text_feat_path)\n",
    "\n",
    "        img_feat_list.append(img_feat_path)\n",
    "        text_feat_list.append(text_feat_path)\n",
    "        done+=1\n",
    "\n",
    "        if done % 500 == 0:\n",
    "            print(f\"Total={total} Done={done}\")\n",
    "\n",
    "    data['img_feat'] = img_feat_list\n",
    "    data['text_feat'] = text_feat_list\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    data.to_json(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 102,007,137\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "clip_model, preprocess = load_clip(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>answer_type</th>\n",
       "      <th>answerable</th>\n",
       "      <th>final_answer</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VizWiz_train_00000000.jpg</td>\n",
       "      <td>What's the name of this product?</td>\n",
       "      <td>[{'answer_confidence': 'yes', 'answer': 'basil...</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>basil leaves</td>\n",
       "      <td>/nfs/home/scg1143/MLDS/Quarter3/DeepLearning/P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VizWiz_train_00000001.jpg</td>\n",
       "      <td>Can you tell me what is in this can please?</td>\n",
       "      <td>[{'answer_confidence': 'yes', 'answer': 'soda'...</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>coca cola</td>\n",
       "      <td>/nfs/home/scg1143/MLDS/Quarter3/DeepLearning/P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image                                     question  \\\n",
       "0  VizWiz_train_00000000.jpg             What's the name of this product?   \n",
       "1  VizWiz_train_00000001.jpg  Can you tell me what is in this can please?   \n",
       "\n",
       "                                             answers answer_type  answerable  \\\n",
       "0  [{'answer_confidence': 'yes', 'answer': 'basil...       other           1   \n",
       "1  [{'answer_confidence': 'yes', 'answer': 'soda'...       other           1   \n",
       "\n",
       "   final_answer                                         image_path  \n",
       "0  basil leaves  /nfs/home/scg1143/MLDS/Quarter3/DeepLearning/P...  \n",
       "1     coca cola  /nfs/home/scg1143/MLDS/Quarter3/DeepLearning/P...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = '../vizviz/vqa/train_df.json'\n",
    "train_data = pd.read_json(train_path)\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path, question, clip_model, transform, device='cpu'):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "    question = clip.tokenize(question).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        text_features = clip_model.encode_text(question)\n",
    "    return image_features.detach().cpu(), text_features.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data.iloc[0]\n",
    "img_feat, text_feat = extract_features(image_path=data.image_path,\n",
    "                                       question=data.question,\n",
    "                                       clip_model=clip_model,\n",
    "                                       transform=preprocess,\n",
    "                                       device='cuda:0'\n",
    "                                    )\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(img_feat,'sample.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_feat,'sample_text.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('project_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90e2368e41498583b2a08aadcd9b50a6cd544d00dc1268d4144554fa52dae42b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
