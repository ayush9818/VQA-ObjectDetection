{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch \n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import clip\n",
    "from tqdm import tqdm \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_MODELS = [\n",
    " 'RN50',\n",
    " 'RN101',\n",
    " 'RN50x4',\n",
    " 'RN50x16',\n",
    " 'RN50x64',\n",
    " 'ViT-B/32',\n",
    " 'ViT-B/16',\n",
    " 'ViT-L/14',\n",
    " 'ViT-L/14@336px']\n",
    "\n",
    "class VizWizDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                df_path, \n",
    "                image_dir, \n",
    "                clip_model,\n",
    "                label2id=None,\n",
    "                id2label=None,\n",
    "                device='cpu',\n",
    "                transform=None):\n",
    "        assert os.path.exists(df_path), f\"{df_path} does not exists\"\n",
    "        self.df = pd.read_csv(df_path)\n",
    "\n",
    "        # using only Answerable Instances\n",
    "        self.df = self.df[self.df.answerable == 1]\n",
    "        \n",
    "\n",
    "        answer_candidates = self.df.final_answer.unique().tolist()\n",
    "        if label2id is None:\n",
    "            self.label2id = {ans:idx for idx,ans in enumerate(answer_candidates)}\n",
    "            self.id2label = {v:k for k,v in self.label2id.items()}\n",
    "        else:\n",
    "            self.label2id = label2id\n",
    "            self.id2label = id2label\n",
    "\n",
    "        self.df = self.df.iloc[:100]\n",
    "        self.n_samples = self.df.shape[0]\n",
    "        self.image_path = self.df[\"image\"].apply(lambda x : os.path.join(image_dir, x))\n",
    "        self.transform = transform\n",
    "        self.clip_model = clip_model \n",
    "        # Initalizing Tensor to Store [Image, Text] Emneddings\n",
    "        self.X = torch.empty((len(self.df), 2048), dtype=torch.float32)\n",
    "        self.device=device\n",
    "        for index in tqdm(range(len(self.df))):\n",
    "            image = Image.open(self.image_path.iloc[index]).convert('RGB')\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image).unsqueeze(0).to(self.device)\n",
    "            question = clip.tokenize(self.df['question'].iloc[index]).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                    image_features = self.clip_model.encode_image(image)\n",
    "                    text_features = self.clip_model.encode_text(question)\n",
    "            self.X[index] = torch.cat((image_features, text_features), 1).to(torch.float32)\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        return index, self.X[index], self.label2id[self.df['final_answer'].iloc[index]], self.df['answerable'].iloc[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "class VQAModelV1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(VQAModelV1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.ln2 = nn.LayerNorm(output_dim)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n",
    "\n",
    "def load_clip(model_name='RN50', device='cpu'):\n",
    "    logger.info(\"Loading CLIP.....\")\n",
    "    assert model_name in CLIP_MODELS, f\"clip models available {CLIP_MODELS}\"\n",
    "    clip_model, preprocess = clip.load(model_name, device=device)\n",
    "    return clip_model, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    base_dir = os.path.dirname(os.getcwd())\n",
    "    data_dir = os.path.join(base_dir, 'vizviz/vqa')\n",
    "    train_image_dir = os.path.join(data_dir, 'train')\n",
    "    val_image_dir = os.path.join(data_dir, 'val')\n",
    "    train_file_path = os.path.join(data_dir, 'train_df.csv')\n",
    "    val_file_path = os.path.join(data_dir, 'eval_df.csv')\n",
    "    batch_size = 32\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(cfg.train_file_path)\n",
    "val_df = pd.read_csv(cfg.val_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1482"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_list = []\n",
    "for ans in val_df.final_answer.unique():\n",
    "    if ans not in train_df.final_answer:\n",
    "        ans_list.append(ans)\n",
    "len(ans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20523, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dog years: memoir' in train_df.final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-20 20:45:04.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_clip\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mLoading CLIP.....\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "clip_model, clip_preprocess = load_clip(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.07it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.52it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VizWizDataset(df_path=cfg.train_file_path,\n",
    "                              image_dir=cfg.train_image_dir,\n",
    "                              clip_model=clip_model,\n",
    "                              device='cuda:0',\n",
    "                              transform=clip_preprocess)\n",
    "\n",
    "val_dataset = VizWizDataset(df_path=cfg.val_file_path,\n",
    "                              image_dir=cfg.val_image_dir,\n",
    "                              clip_model=clip_model,\n",
    "                              device='cuda:0',\n",
    "                              transform=clip_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=cfg.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQAModelV1(input_dim=2048, hidden_dim=2048, output_dim=len(train_dataset.label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_accuracy(train_df, id2label, train_index, pred_index):\n",
    "    pred_label = id2label[int(pred_index)]\n",
    "    train_row = train_df.iloc[int(train_index)]\n",
    "    answer_set = np.array([ans['answer'] for ans in json.loads(train_row['answers'].replace(\"\\'\", \"\\\"\"))])\n",
    "\n",
    "    score = min(1, len(np.where(answer_set == pred_label)[0]) / 3)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/scg1143/.conda/envs/project_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "LEARNING_RATE=0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4, verbose=False)\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqa(model, data_loader, id2label, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    accuracy = 0\n",
    "    for i, (index, x, answers, _) in enumerate(data_loader):\n",
    "        x = x.to(device) \n",
    "        answers = answers.to(device)\n",
    "        # Forward Pass\n",
    "        outputs = model(x).squeeze(1)\n",
    "        print(outputs.dtype, answers.dtype, outputs.shape, answers.shape)\n",
    "        loss = criterion(outputs, answers)\n",
    "        \n",
    "        # Backward Pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Loss and Accuracy Calculations\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        for ip, idx in enumerate(index):\n",
    "            accuracy += vqa_accuracy(data_loader.dataset.df, id2label, int(idx), predicted[ip])\n",
    "\n",
    "    # id2label, train_index, pred_index\n",
    "    train_loss /= len(data_loader.dataset)\n",
    "    accuracy /= len(data_loader.dataset)\n",
    "    \n",
    "    return train_loss, accuracy\n",
    "\n",
    "def validate_vqa(model, data_loader, id2label, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (index, x, answers, _) in enumerate(data_loader):\n",
    "            x = x.to(device)\n",
    "            answers = answers.to(device)\n",
    "            # Forward Pass\n",
    "            outputs = model(x).squeeze(1)\n",
    "            loss = criterion(outputs, answers)\n",
    "            \n",
    "            # Loss and Accuracy Calculations\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            for ip, idx in enumerate(index):\n",
    "                accuracy += vqa_accuracy(data_loader.dataset.df, id2label, int(idx), predicted[ip])\n",
    "\n",
    "    val_loss /= len(data_loader.dataset)\n",
    "    accuracy /= len(data_loader.dataset)\n",
    "\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]:\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([4, 79]) torch.Size([4])\n",
      "val_loss improved from inf to 0.17637\n",
      "0s 14.8918ms/step - loss: 0.1588 - accuracy: 27.0000% - val_loss: 0.1764 - val_accuracy: 34.6667% - lr: 0.0001\n",
      "\n",
      "Epoch [2/10]:\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([4, 79]) torch.Size([4])\n",
      "0s 7.0025ms/step - loss: 0.1520 - accuracy: 25.0000% - val_loss: 0.1766 - val_accuracy: 33.3333% - lr: 0.0001\n",
      "\n",
      "Epoch [3/10]:\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([4, 79]) torch.Size([4])\n",
      "0s 6.5027ms/step - loss: 0.1541 - accuracy: 28.0000% - val_loss: 0.1770 - val_accuracy: 32.6667% - lr: 0.0001\n",
      "\n",
      "Epoch [4/10]:\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([4, 79]) torch.Size([4])\n",
      "0s 6.4021ms/step - loss: 0.1442 - accuracy: 31.0000% - val_loss: 0.1771 - val_accuracy: 32.6667% - lr: 1e-05\n",
      "\n",
      "Epoch [5/10]:\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([4, 79]) torch.Size([4])\n",
      "0s 6.2573ms/step - loss: 0.1492 - accuracy: 29.3333% - val_loss: 0.1772 - val_accuracy: 32.6667% - lr: 1e-05\n",
      "\n",
      "Epoch [6/10]:\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([32, 79]) torch.Size([32])\n",
      "torch.float32 torch.int64 torch.Size([4, 79]) torch.Size([4])\n",
      "val_loss hasn't improved for 5 epochs. Early stopping.\n"
     ]
    }
   ],
   "source": [
    "# Defining Lists to store training and validation accuracies and losses\n",
    "train_vqa_acc_history = []\n",
    "train_vqa_loss_history = []\n",
    "val_vqa_acc_history = []\n",
    "val_vqa_loss_history = []\n",
    "\n",
    "counter = 0\n",
    "NUM_EPOCHS=10\n",
    "patience=5\n",
    "model.to(device)\n",
    "best_val_loss=float('inf')\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}]:\")\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    train_loss, train_acc = train_vqa(model, train_dataloader, train_dataset.id2label, criterion, optimizer)\n",
    "    val_loss, val_acc = validate_vqa(model, val_dataloader, train_dataset.id2label, criterion)\n",
    "    \n",
    "    epoch_time = time.perf_counter() - start_time\n",
    "    avg_step_time = epoch_time / (len(train_dataloader) + len(val_dataloader))\n",
    "        \n",
    "    train_vqa_acc_history.append(train_acc)\n",
    "    train_vqa_loss_history.append(train_loss)\n",
    "    val_vqa_acc_history.append(val_acc)\n",
    "    val_vqa_loss_history.append(val_loss)\n",
    "    \n",
    "     # Check if the validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"val_loss improved from {best_val_loss:.5f} to {val_loss:.5f}\")\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"val_loss hasn't improved for {patience} epochs. Early stopping.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"{int(np.round(epoch_time))}s {avg_step_time*1e3:.4f}ms/step - loss: {train_loss:.4f} - accuracy: {train_acc*100:.4f}% - val_loss: {val_loss:.4f} - val_accuracy: {val_acc*100:.4f}% - lr: {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    lr_scheduler.step(val_loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.sigmoid(outputs).max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(logits)):\n",
    "    if logits[i] == batch_[2][i]:\n",
    "        print(i)\n",
    "        correct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grey'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.id2label[int(logits[10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'black dog', 'dog']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = int(batch_[0][10])\n",
    "row = train_dataset.df.iloc[idx]\n",
    "answer_set = [ans['answer'] for ans in json.loads(row['answers'].replace(\"\\'\", \"\\\"\"))]\n",
    "answer_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_set = np.array(answer_set)\n",
    "min(1, len(np.where(answer_set == 'grey')[0]) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('project_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90e2368e41498583b2a08aadcd9b50a6cd544d00dc1268d4144554fa52dae42b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
