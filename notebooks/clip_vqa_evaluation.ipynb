{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip \n",
    "import os \n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = '../vizviz/vqa/val_df_thresh_3.csv'\n",
    "train_data_path = '../vizviz/vqa/train_df_thresh_3.csv'\n",
    "image_dir = '../vizviz/vqa/train'\n",
    "\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "test_df['image_path'] = test_df['image'].apply(lambda x : os.path.abspath(os.path.join(image_dir, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_candidates = train_df[train_df.answerable == 1].final_answer.unique().tolist()\n",
    "label2id = {ans:idx for idx,ans in enumerate(answer_candidates)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "with open('../vizviz/vqa/label2id.json','w') as f:\n",
    "    json.dump(label2id,f)\n",
    "\n",
    "with open('../vizviz/vqa/id2label.json','w') as f:\n",
    "    json.dump(id2label,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    clip_model_name = \"RN50\"\n",
    "    id2label = '../vizviz/vqa/id2label.json'\n",
    "    model_path = '../runs/vqa_clip_model/1/clip_vqa_0.632963594994311.pth'\n",
    "    hidden_dim = 2048\n",
    "    input_dim = 2048\n",
    "    output_dim = 821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import VQAModelV1\n",
    "\n",
    "class VQAModule:\n",
    "    CLIP_MODELS = [\n",
    "            \"RN50\",\n",
    "            \"RN101\",\n",
    "            \"RN50x4\",\n",
    "            \"RN50x16\",\n",
    "            \"RN50x64\",\n",
    "            \"ViT-B/32\",\n",
    "            \"ViT-B/16\",\n",
    "            \"ViT-L/14\",\n",
    "            \"ViT-L/14@336px\",\n",
    "        ]\n",
    "    def __init__(self, model_config, device=\"cpu\"):\n",
    "        self.model_config = model_config\n",
    "        self.device=device \n",
    "        self.clip_model, self.preprocess = self.load_clip(model_name=self.model_config.clip_model_name, device=self.device)\n",
    "        print(\"Clip Loaded\")\n",
    "\n",
    "        self.id2label = json.load(open(model_config.id2label))\n",
    "        self.vqa_model = torch.load(model_config.model_path).to(self.device)\n",
    "        self.vqa_model.eval()\n",
    "        \n",
    "    def load_clip(self, model_name=\"RN50\", device=\"cpu\"):\n",
    "        assert model_name in self.CLIP_MODELS, f\"clip models available {self.CLIP_MODELS}\"\n",
    "        clip_model, preprocess = clip.load(model_name, device=device)\n",
    "        return clip_model, preprocess\n",
    "\n",
    "    def predict(self, question : str, image_path : str):\n",
    "        assert os.path.exists(image_path), f\"{image_path} does not exists\"\n",
    "        image = self.preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(self.device)\n",
    "        question = clip.tokenize(question).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(image)\n",
    "            text_features = self.clip_model.encode_text(question)\n",
    "            vqa_input = torch.cat((image_features, text_features), 1).to(torch.float32)\n",
    "            outputs = self.vqa_model(vqa_input)\n",
    "            _, index = outputs.max(1)\n",
    "        pred_index = int(index.detach().cpu())\n",
    "        answer = self.id2label[str(pred_index)]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip Loaded\n"
     ]
    }
   ],
   "source": [
    "vqa = VQAModule(model_config=ModelConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
