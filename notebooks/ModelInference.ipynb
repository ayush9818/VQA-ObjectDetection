{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model='dandelin/vilt-b32-mlm'\n",
    "model_path = \"/nfs/home/scg1143/MLDS/Quarter3/DeepLearning/Project/VQA-ObjectDetection/runs/1/model_state_15.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir=os.path.abspath('../dataset/images')\n",
    "eval_file_path = os.path.abspath('../dataset/data_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2494, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the colour of the bag on the chair</td>\n",
       "      <td>pink</td>\n",
       "      <td>image399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is at the right bottom</td>\n",
       "      <td>table</td>\n",
       "      <td>image1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what are found on the rack</td>\n",
       "      <td>toy</td>\n",
       "      <td>image1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is left of printer</td>\n",
       "      <td>mirror</td>\n",
       "      <td>image529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is the colour of television</td>\n",
       "      <td>black</td>\n",
       "      <td>image201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     question  answer   image_id\n",
       "0  what is the colour of the bag on the chair    pink   image399\n",
       "1                 what is at the right bottom   table  image1341\n",
       "2                  what are found on the rack     toy  image1320\n",
       "3                     what is left of printer  mirror   image529\n",
       "4            what is the colour of television   black   image201"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(eval_file_path)\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['image_path'] = test_df['image_id'].apply(lambda x : os.path.join(image_dir, f\"{x}.png\"))\n",
    "assert os.path.exists(test_df.image_path.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAPredictor:\n",
    "    def __init__(self, base_model, model_path, device='cpu'):\n",
    "        self.processor = ViltProcessor.from_pretrained(base_model)\n",
    "        assert os.path.exists(model_path), f\"{model_path} does not exists\"\n",
    "        model_dict = torch.load(model_path)\n",
    "        print(f\"Accuracy : {model_dict['best_epoch_acc']}\")\n",
    "        self.id2label = model_dict['id2label']\n",
    "        self.label2id = model_dict['label2id']\n",
    "        self.device = device\n",
    "        self.model = ViltForQuestionAnswering.from_pretrained(base_model, id2label=self.id2label, label2id=self.label2id)\n",
    "        self.model.load_state_dict(model_dict['state_dict'])\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        print(\"Model Loaded\")\n",
    "\n",
    "    def predict(self, image_path, text, topk=3):\n",
    "        image = Image.open(image_path)\n",
    "        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        pixel_mask = self.processor.image_processor.pad(encoding['pixel_values'], return_tensors=\"pt\")['pixel_mask']\n",
    "        encoding['pixel_mask'] = pixel_mask\n",
    "        encoding.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoding)\n",
    "            logits = torch.sigmoid(outputs.logits)\n",
    "            logits = logits.detach().cpu().numpy()[0]\n",
    "            sorted_indices = np.argsort(logits)[::-1]\n",
    "            sorted_probs = logits[sorted_indices]\n",
    "        result = [ {\"answer\" : self.id2label[sorted_indices[k]], \"prob\" : sorted_probs[k]} for k in range(topk)]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 86.0523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.1.bias', 'classifier.1.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "predictor = VQAPredictor(base_model=base_model, model_path=model_path, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'photo', 'prob': 0.041434072},\n",
       " {'answer': 'papers', 'prob': 0.015667612},\n",
       " {'answer': 'toy', 'prob': 0.008851868}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = test_df.iloc[6]\n",
    "out = predictor.predict(image_path = test_case['image_path'], text=test_case['question'])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question                          what is the ball on the table\n",
       "answer                                               basketball\n",
       "image_id                                               image477\n",
       "image_path    /nfs/home/scg1143/MLDS/Quarter3/DeepLearning/P...\n",
       "Name: 6, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'pixel_mask'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx : 0 Num Corrects : 0\n",
      "Idx : 200 Num Corrects : 56\n",
      "Idx : 400 Num Corrects : 109\n",
      "Idx : 600 Num Corrects : 170\n",
      "Idx : 800 Num Corrects : 248\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/project_env/lib/python3.9/site-packages/transformers/feature_extraction_utils.py:182\u001b[0m, in \u001b[0;36mBatchFeature.convert_to_tensors\u001b[0;34m(self, tensor_type)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 182\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    184\u001b[0m     \u001b[39mself\u001b[39m[key] \u001b[39m=\u001b[39m tensor\n",
      "File \u001b[0;32m~/.conda/envs/project_env/lib/python3.9/site-packages/transformers/feature_extraction_utils.py:141\u001b[0m, in \u001b[0;36mBatchFeature._get_is_as_tensor_fns.<locals>.as_tensor\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    140\u001b[0m     value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(value)\n\u001b[0;32m--> 141\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m image_path \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mimage_path\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m text\u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m result \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mpredict(image_path \u001b[39m=\u001b[39;49m image_path, text\u001b[39m=\u001b[39;49mtext)\n\u001b[1;32m      6\u001b[0m \u001b[39mif\u001b[39;00m result[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      7\u001b[0m     num_corrects\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[73], line 18\u001b[0m, in \u001b[0;36mVQAPredictor.predict\u001b[0;34m(self, image_path, text, topk)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, image_path, text, topk\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[1;32m     17\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(image_path)\n\u001b[0;32m---> 18\u001b[0m     encoding \u001b[39m=\u001b[39m processor(image, text, padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m     encoding\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     20\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.conda/envs/project_env/lib/python3.9/site-packages/transformers/models/vilt/processing_vilt.py:109\u001b[0m, in \u001b[0;36mViltProcessor.__call__\u001b[0;34m(self, images, text, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m     91\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m     92\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[39m# add pixel_values + pixel_mask\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m encoding_image_processor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_processor(images, return_tensors\u001b[39m=\u001b[39;49mreturn_tensors)\n\u001b[1;32m    110\u001b[0m encoding\u001b[39m.\u001b[39mupdate(encoding_image_processor)\n\u001b[1;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m encoding\n",
      "File \u001b[0;32m~/.conda/envs/project_env/lib/python3.9/site-packages/transformers/image_processing_utils.py:551\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m    550\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/project_env/lib/python3.9/site-packages/transformers/models/vilt/image_processing_vilt.py:499\u001b[0m, in \u001b[0;36mViltImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, size_divisor, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_pad, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m images \u001b[39m=\u001b[39m [\n\u001b[1;32m    495\u001b[0m     to_channel_dimension_format(image, data_format, input_channel_dim\u001b[39m=\u001b[39minput_data_format) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    496\u001b[0m ]\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m do_pad:\n\u001b[0;32m--> 499\u001b[0m     encoded_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    500\u001b[0m         images, return_pixel_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49mreturn_tensors, input_data_format\u001b[39m=\u001b[39;49mdata_format\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     encoded_outputs \u001b[39m=\u001b[39m BatchFeature(data\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}, tensor_type\u001b[39m=\u001b[39mreturn_tensors)\n",
      "File \u001b[0;32m~/.conda/envs/project_env/lib/python3.9/site-packages/transformers/models/vilt/image_processing_vilt.py:352\u001b[0m, in \u001b[0;36mViltImageProcessor.pad\u001b[0;34m(self, images, constant_values, return_pixel_mask, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    346\u001b[0m     masks \u001b[39m=\u001b[39m [\n\u001b[1;32m    347\u001b[0m         make_pixel_mask(image\u001b[39m=\u001b[39mimage, output_size\u001b[39m=\u001b[39mpad_size, input_data_format\u001b[39m=\u001b[39minput_data_format)\n\u001b[1;32m    348\u001b[0m         \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    349\u001b[0m     ]\n\u001b[1;32m    350\u001b[0m     data[\u001b[39m\"\u001b[39m\u001b[39mpixel_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m masks\n\u001b[0;32m--> 352\u001b[0m \u001b[39mreturn\u001b[39;00m BatchFeature(data\u001b[39m=\u001b[39;49mdata, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/.conda/envs/project_env/lib/python3.9/site-packages/transformers/feature_extraction_utils.py:78\u001b[0m, in \u001b[0;36mBatchFeature.__init__\u001b[0;34m(self, data, tensor_type)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, tensor_type: Union[\u001b[39mNone\u001b[39;00m, \u001b[39mstr\u001b[39m, TensorType] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     77\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(data)\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type)\n",
      "File \u001b[0;32m~/.conda/envs/project_env/lib/python3.9/site-packages/transformers/feature_extraction_utils.py:188\u001b[0m, in \u001b[0;36mBatchFeature.convert_to_tensors\u001b[0;34m(self, tensor_type)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_values\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    187\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing values of different lengths. \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 188\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    189\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate padding \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "num_corrects = 0\n",
    "for idx,row in test_df.iterrows():\n",
    "    image_path = row['image_path']\n",
    "    text= row['question']\n",
    "    result = predictor.predict(image_path = image_path, text=text)\n",
    "    if result[0]['answer'] == row['answer']:\n",
    "        num_corrects+=1\n",
    "\n",
    "    if idx % 200 == 0:\n",
    "        print(f\"Idx : {idx} Num Corrects : {num_corrects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question                       how many plastic boxes are there\n",
       "answer                                                        2\n",
       "image_id                                               image447\n",
       "image_path    /nfs/home/scg1143/MLDS/Quarter3/DeepLearning/P...\n",
       "Name: 157, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('project_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90e2368e41498583b2a08aadcd9b50a6cd544d00dc1268d4144554fa52dae42b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
